{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "#define global variables: FL Node amount, batch size, learning rate, epochs\n",
    "global FL_NODE_AMOUNT \n",
    "global BATCH_64\n",
    "global BATCH_128\n",
    "global LEARNING_RATE\n",
    "global EPOCHS\n",
    "\n",
    "FL_NODE_AMOUNT = 5\n",
    "BATCH_64 = 64\n",
    "BATCH_128 = 128\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab dataset from MNIST and use random_split to seperate the dataset into parts for federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist(data_path: str = './data'):\n",
    "    '''This function downloads the MNIST dataset into the `data_path`\n",
    "    directory if it is not there already. WE construct the train/test\n",
    "    split by converting the images into tensors and normalising them'''\n",
    "    \n",
    "    # transformation to convert images to tensors and apply normalisation\n",
    "    tr = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    # prepare train and test set\n",
    "    trainset = MNIST(data_path, train=True, download=True, transform=tr)\n",
    "    testset = MNIST(data_path, train=False, download=True, transform=tr)\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "def prepare_dataset(num_partitions: int,\n",
    "                    batch_size: int,\n",
    "                    val_ratio: float = 0.1):\n",
    "\n",
    "    \"\"\"This function partitions the training set into N disjoint\n",
    "    subsets, each will become the local dataset of a client. This\n",
    "    function also subsequently partitions each traininset partition\n",
    "    into train and validation. The test set is left intact and will\n",
    "    be used by the central server to asses the performance of the\n",
    "    global model. \"\"\"\n",
    "\n",
    "    # get the MNIST dataset\n",
    "    trainset, testset = get_mnist()\n",
    "\n",
    "    # split trainset into `num_partitions` trainsets\n",
    "    num_images = len(trainset) // num_partitions\n",
    "\n",
    "    partition_len = [num_images] * num_partitions\n",
    "\n",
    "    trainsets = random_split(trainset, partition_len, torch.Generator().manual_seed(2023))\n",
    "\n",
    "    # create dataloaders with train+val support\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for trainset_ in trainsets:\n",
    "        num_total = len(trainset_)\n",
    "        num_val = int(val_ratio * num_total)\n",
    "        num_train = num_total - num_val\n",
    "\n",
    "        for_train, for_val = random_split(trainset_, [num_train, num_val], torch.Generator().manual_seed(2023))\n",
    "\n",
    "        trainloaders.append(DataLoader(for_train, batch_size=batch_size, shuffle=True, num_workers=2))\n",
    "        valloaders.append(DataLoader(for_val, batch_size=batch_size, shuffle=False, num_workers=2))\n",
    "\n",
    "    # create dataloader for the test set\n",
    "    testloader = DataLoader(testset, batch_size=128)\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def train(net, trainloader, optimizer, epochs):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    net.train()\n",
    "    for _ in range(epochs):\n",
    "        model_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model_loss += loss.item() / len(trainloader)\n",
    "        #set the loss to have 4 decimal places\n",
    "        model_loss = round(model_loss, 4)\n",
    "    return net, model_loss\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    \n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def run_centralised(trainloader, testloader, epochs: int, lr: float, momentum: float=0.9):\n",
    "    \"\"\"A minimal (but complete) training loop\"\"\"\n",
    "    # instantiate the model\n",
    "    model = Net()\n",
    "    print(\"Model initialised\")\n",
    "    # define optimiser with hyperparameters supplied\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    # get dataset and construct a dataloaders\n",
    "    \n",
    "    print(\"@train test loaders all clear@\")\n",
    "    # train for the specified number of \n",
    "    print(\"training process stating...\")\n",
    "    trained_model, loss = train(model, trainloader, optim, epochs)\n",
    "    print(\"@training completed@\")\n",
    "    # training is completed, then evaluate model on the test set\n",
    "    print(\"testing process starting ...\")\n",
    "    accuracy = test(trained_model, testloader)\n",
    "    print(\"@testing  completed@\")\n",
    "    print(f\"{loss = }\")\n",
    "    print(f\"{accuracy = }\")\n",
    "    return loss, accuracy, trained_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_federated_averaging(model_sets, weights):\n",
    "    \"\"\"\n",
    "    Weighted federated averaging.\n",
    "\n",
    "    Args:\n",
    "        model_sets (list of model): The list containing the models from each client.\n",
    "        weights (list of float): The list containing the weights for each model, which could be based on their loss or accuracy.\n",
    "\n",
    "    Returns:\n",
    "        global_model (model): The global model after weighted federated averaging.\n",
    "    \"\"\"\n",
    "\n",
    "    global_model = Net()\n",
    "\n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [weight / total_weight for weight in weights]\n",
    "\n",
    "    # Ensure the global model parameters are initialized to zero\n",
    "    for global_param in global_model.parameters():\n",
    "        global_param.data *= 0 \n",
    "\n",
    "    # Accumulate weighted parameters from each model\n",
    "    for model, weight in zip(model_sets, normalized_weights):\n",
    "        for global_param, model_param in zip(global_model.parameters(), model.parameters()):\n",
    "            global_param.data += model_param.data * weight\n",
    "            \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = get_mnist()\n",
    "\n",
    "\n",
    "print(\"tradition CNN model for MNIST STARTS\")\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_64, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_128)\n",
    "\n",
    "loss, accracy, trained_model= run_centralised(trainloader,testloader,epochs=EPOCHS, lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "print(\"\\n------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL process with CNN for MNIST STARTS\n",
      "@Data preparation completed@\n",
      "Training client 0\n",
      "Model initialised\n",
      "@train test loaders all clear@\n",
      "training process stating...\n",
      "@training completed@\n",
      "testing process starting ...\n",
      "@testing  completed@\n",
      "loss = 0.0246\n",
      "accuracy = 0.9764\n",
      "Client 0 training completed\n",
      "\n",
      "Training client 1\n",
      "Model initialised\n",
      "@train test loaders all clear@\n",
      "training process stating...\n",
      "@training completed@\n",
      "testing process starting ...\n",
      "@testing  completed@\n",
      "loss = 0.0344\n",
      "accuracy = 0.9792\n",
      "Client 1 training completed\n",
      "\n",
      "Training client 2\n",
      "Model initialised\n",
      "@train test loaders all clear@\n",
      "training process stating...\n",
      "@training completed@\n",
      "testing process starting ...\n",
      "@testing  completed@\n",
      "loss = 0.037\n",
      "accuracy = 0.9796\n",
      "Client 2 training completed\n",
      "\n",
      "Training client 3\n",
      "Model initialised\n",
      "@train test loaders all clear@\n",
      "training process stating...\n",
      "@training completed@\n",
      "testing process starting ...\n",
      "@testing  completed@\n",
      "loss = 0.0302\n",
      "accuracy = 0.9779\n",
      "Client 3 training completed\n",
      "\n",
      "Training client 4\n",
      "Model initialised\n",
      "@train test loaders all clear@\n",
      "training process stating...\n",
      "@training completed@\n",
      "testing process starting ...\n",
      "@testing  completed@\n",
      "loss = 0.0268\n",
      "accuracy = 0.9775\n",
      "Client 4 training completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"FL process with CNN for MNIST STARTS\")\n",
    "trainloaders, valloaders, testloader = prepare_dataset(num_partitions=FL_NODE_AMOUNT, batch_size=BATCH_64)\n",
    "print(\"@Data preparation completed@\")\n",
    "\n",
    "model_sets =[]\n",
    "accuracy_sets = []\n",
    "loss_sets = [] \n",
    "for client_index in range(FL_NODE_AMOUNT):\n",
    "    print(f\"Training client {client_index}\")\n",
    "    \n",
    "    trainloader = trainloaders[client_index]\n",
    "    # partial_model = Net(num_classes=10)\n",
    "    # partial_optimizer = torch.optim.SGD(partial_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "    # partial_model = train(trained_model, trainloader,partial_optimizer , EPOCHS)\n",
    "    loss,accracy,partial_model = run_centralised(trainloader, testloader, epochs=EPOCHS, lr=LEARNING_RATE)\n",
    "    model_sets.append(partial_model)\n",
    "    accuracy_sets.append(accracy)\n",
    "    loss_sets.append(loss)\n",
    "    \n",
    "    \n",
    "    print(f\"Client {client_index} training completed\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9764, 0.9792, 0.9796, 0.9779, 0.9775]\n",
      "Weighted federated averaging Starting ...\n",
      "calculating accuracy of the global model ...\n",
      "Global model accuracy: 0.1781\n",
      "FL process with CNN for MNIST ENDS\n"
     ]
    }
   ],
   "source": [
    "#finally I get the average of the weights of the trained models\n",
    "# this is the global model\n",
    "print(accuracy_sets)\n",
    "print(\"Weighted federated averaging Starting ...\")\n",
    "global_model = weighted_federated_averaging(model_sets, accuracy_sets)\n",
    "print(\"calculating accuracy of the global model ...\")\n",
    "accuracy = test(global_model, testloader)\n",
    "\n",
    "print(f\"Global model accuracy: {accuracy}\")\n",
    "print(\"FL process with CNN for MNIST ENDS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
