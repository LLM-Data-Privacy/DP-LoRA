import sys
import os

os.environ['TRANSFORMERS_CACHE'] = '/gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface'
os.environ['HF_DATASETS_CACHE'] = '/gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface'
os.environ['HF_HOME'] = '/gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface'

# Ensure the directory exists
os.makedirs('/gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface', exist_ok=True)

from huggingface_hub import login

# os.environ['HF_DATASETS_CACHE'] = '/gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface'
# os.environ['HF_HOME'] = '/gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface'

# os.environ['export TRANSFORMERS_CACHE = /gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface']
# os.environ['export HF_DATASETS_CACHE = /gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface']
# os.environ['export HF_HOME = /gpfs/u/home/FNAI/FNAIjspr/scratch/huggingface']

# Verify environment variables
print("TRANSFORMERS_CACHE:", os.getenv('TRANSFORMERS_CACHE'))
print("HF_DATASETS_CACHE:", os.getenv('HF_DATASETS_CACHE'))
print("HF_HOME:", os.getenv('HF_HOME'))

# Log in to the Hugging Face Hub
TOKEN = "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"
login(token=TOKEN)

import pandas as pd

import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
# from config import *

from sklearn.metrics import accuracy_score, f1_score
from datasets import load_dataset, load_from_disk, Dataset
from tqdm import tqdm
import datasets
import torch

from torch.utils.data import DataLoader
from functools import partial
from pathlib import Path
import warnings


def parseData():
    instructions = load_dataset("TheFinAI/flare-ner")
    test = instructions["test"].to_pandas()
    return test

def get_model(model_name="mistralai/Mistral-7B-Instruct-v0.1"):
    print("Loading model...")
    model = AutoModelForCausalLM.from_pretrained(model_name)
    print("Model loaded successfully")

    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token_id = tokenizer.eos_token_id
    print("Tokenizer loaded successfully")

    return model, tokenizer

def NER(model, tokenizer):
    batch_size = 4
    test = parseData()

    batches = [(i, min(i + batch_size, len(test))) for i in range(0, len(test), batch_size)]
    print(f"Total len: {len(test['query'])}. Batchsize: {batch_size}. Total steps: {len(batches)}")

    out_text_list = []
    full_output = []
    for i in range(0, len(test), batch_size):
        tmp_context = list(test['query'][i : min(i + batch_size, len(test))])
        
        tokens = tokenizer(tmp_context, return_tensors='pt', padding=True, max_length=512, return_token_type_ids=False)
        for k in tokens.keys():
            tokens[k] = tokens[k].cuda()

        res = model.generate(**tokens, max_length=1024, temperature = 0.7, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)
        res_sentences = [tokenizer.decode(j, skip_special_tokens=True) for j in res]
        
        print("\n\n--------------------", i/4, "------------------------") 
        for o in res_sentences:
            print(o.split("Answer:")[1])
            print("------------------------------------------------") 
        out_text = [o.split("Answer:")[1] for o in res_sentences]
        
        full_text = [o for o in res_sentences]
        full_output += full_text
        out_text_list += out_text
        torch.cuda.empty_cache()

    stats = pd.DataFrame({'query': test['query'], 'answer': gt, 'output': out_text_list, 'full_output': full_output})
    print(stats)

    stats.to_csv('/gpfs/u/home/FNAI/FNAIjspr/barn/DP-LoRA/mistral7bNER/results.csv', encoding='utf-8', index=False)

    acc = accuracy_score(stats['new_answer'], stats['new_output'])
    f1_macro = f1_score(stats['new_answer'], stats['new_output'], average = "macro")
    f1_micro = f1_score(stats['new_answer'], stats['new_output'], average = "micro")
    f1_weighted = f1_score(stats['new_answer'], stats['new_output'], average = "weighted")

    metrics = f"Acc: {acc}. F1 macro: {f1_macro}. F1 micro: {f1_micro}. F1 weighted: {f1_weighted}.\n"
    output_file = '/gpfs/u/home/FNAI/FNAIjspr/barn/DP-LoRA/mistral7bNER/results_stats.txt'
    with open(output_file, 'w') as f:
        f.write(metrics)

if __name__ == "__main__":
    warnings.filterwarnings('ignore')
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print("Transformer Version:", transformers.__version__)
    print("Device:", device)

    model, tokenizer = get_model()
    model.to(device)
    NER(model, tokenizer)

    # example_prompt = "Analyze the sentiment of this statement extracted from a financial news article. Provide your answer as either negative, positive, or neutral. Text: The company expects its net sales for the whole 2009 to remain below the 2008 level . Answer:"
    # example_prompt = "Analyze the sentiment of this statement extracted from a financial news article. Provide your answer as either negative, positive, or neutral. Text: According to Sepp+Ã±nen , the new technology UMTS900 solution network building costs are by one-third lower than that of the building of 3.5 G networks , operating at 2,100 MHz frequency . Answer:"
    # test_prompt = query[3]
    # inputs = tokenizer(test_prompt, return_tensors="pt", padding=True, max_length=512, truncation=True, return_token_type_ids=False).to(device)
    # # print(inputs)
    # res = model.generate(**inputs, max_new_tokens=100, do_sample=True, eos_token_id=tokenizer.eos_token_id)
    # print(tokenizer.batch_decode(res)[0])
    # print("Ground Truth Answer:", gt[3])


    # scp D:/Study/Testing/tester.py FNAIjspr@blp01.ccni.rpi.edu:/gpfs/u/home/FNAI/FNAIjspr/barn/DP-LoRA/mistral7bNER
