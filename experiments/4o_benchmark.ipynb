{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Library needed for model and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "TOKEN = \"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\"\n",
    "login(token = TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. get pretrained model and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",token = TOKEN).to(device) \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",token = TOKEN)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. load  datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "dataset = load_dataset(\"TheFinAI/flare-fpb\",token = TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['id', 'query', 'answer', 'text', 'choices', 'gold'], 'validation': ['id', 'query', 'answer', 'text', 'choices', 'gold'], 'test': ['id', 'query', 'answer', 'text', 'choices', 'gold']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.column_names)\n",
    "def preprocess_function(examples):\n",
    "    combined_texts = [q + \" [SEP] \" + t for q, t in zip(examples['query'], examples['text'])]\n",
    "    return tokenizer(combined_texts, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['fpb0', 'fpb1'], 'query': ['Analyze the sentiment of this statement extracted from a financial news article. Provide your answer as either negative, positive, or neutral.\\nText: The five-storey , eco-efficient building will have a gross floor area of about 15,000 sq m. It will also include apartments .\\nAnswer:', 'Analyze the sentiment of this statement extracted from a financial news article. Provide your answer as either negative, positive, or neutral.\\nText: According to Sepp+ñnen , the new technology UMTS900 solution network building costs are by one-third lower than that of the building of 3.5 G networks , operating at 2,100 MHz frequency .\\nAnswer:'], 'answer': ['neutral', 'positive'], 'text': ['The five-storey , eco-efficient building will have a gross floor area of about 15,000 sq m. It will also include apartments .', 'According to Sepp+ñnen , the new technology UMTS900 solution network building costs are by one-third lower than that of the building of 3.5 G networks , operating at 2,100 MHz frequency .'], 'choices': [['positive', 'neutral', 'negative'], ['positive', 'neutral', 'negative']], 'gold': [1, 0], 'input_ids': [[1, 1094, 8910, 1374, 272, 21790, 302, 456, 6251, 25081, 477, 264, 5593, 4231, 5447, 28723, 7133, 547, 574, 4372, 390, 2477, 7087, 28725, 5278, 28725, 442, 14214, 28723, 13, 1874, 28747, 415, 3359, 28733, 5987, 28724, 1200, 317, 1115, 28733, 28627, 3667, 622, 506, 264, 17725, 4366, 2698, 302, 684, 28705, 28740, 28782, 28725, 28734, 28734, 28734, 18328, 290, 28723, 661, 622, 835, 3024, 28123, 842, 13, 2820, 16981, 28747, 733, 1151, 28753, 28793, 415, 3359, 28733, 5987, 28724, 1200, 317, 1115, 28733, 28627, 3667, 622, 506, 264, 17725, 4366, 2698, 302, 684, 28705, 28740, 28782, 28725, 28734, 28734, 28734, 18328, 290, 28723, 661, 622, 835, 3024, 28123, 842], [1, 1094, 8910, 1374, 272, 21790, 302, 456, 6251, 25081, 477, 264, 5593, 4231, 5447, 28723, 7133, 547, 574, 4372, 390, 2477, 7087, 28725, 5278, 28725, 442, 14214, 28723, 13, 1874, 28747, 6586, 298, 1091, 587, 28806, 28877, 4588, 1200, 272, 633, 5514, 500, 7333, 28735, 28774, 28734, 28734, 5165, 3681, 3667, 6966, 460, 486, 624, 28733, 16507, 3889, 821, 369, 302, 272, 3667, 302, 28705, 28770, 28723, 28782, 420, 12167, 1200, 10513, 438, 28705, 28750, 28725, 28740, 28734, 28734, 351, 12659, 11010, 842, 13, 2820, 16981, 28747, 733, 1151, 28753, 28793, 6586, 298, 1091, 587, 28806, 28877, 4588, 1200, 272, 633, 5514, 500, 7333, 28735, 28774, 28734, 28734, 5165, 3681, 3667, 6966, 460, 486, 624, 28733, 16507, 3889, 821, 369, 302, 272, 3667, 302, 28705, 28770, 28723, 28782, 420, 12167, 1200, 10513, 438, 28705, 28750, 28725, 28740, 28734, 28734, 351, 12659, 11010, 842]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(processed_dataset['train'][0:2])\n",
    "input_ids = processed_dataset['train']['input_ids']\n",
    "attention_mask = processed_dataset['train']['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. define testing indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1(predictions, references):\n",
    "    f1 = f1_score(references, predictions, average='weighted')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. run the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 110 at dim 1 (got 144)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(attention_mask)\n\u001b[0;32m      6\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 110 at dim 1 (got 144)"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "\n",
    "inputs = torch.tensor(input_ids)\n",
    "masks = torch.tensor(attention_mask)\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs, attention_mask=masks)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. analyze and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Elapsed time for inference: {elapsed_time} seconds\")\n",
    "\n",
    "actual_labels = dataset['train']['gold'] \n",
    "f1 = compute_f1(predictions.numpy(), actual_labels)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
