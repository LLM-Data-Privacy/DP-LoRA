{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Initialize Directories\n",
    "This block check if the certain path exists. If they do, then remove the corresponding path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "partition_size = 5\n",
    "\n",
    "jsonl_path = './data/dataset_new.jsonl'\n",
    "save_path = './data/dataset_new'\n",
    "partition_path = './data/partition'\n",
    "\n",
    "if os.path.exists(jsonl_path):\n",
    "    os.remove(jsonl_path)\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "directory = \"./data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and Prepare Dataset:\n",
    "- Import necessary libraries from the datasets package: https://huggingface.co/docs/datasets/index\n",
    "- Load the Twitter Financial News Sentiment (TFNS) dataset and convert it to a Pandas dataframe. https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment\n",
    "- Map numerical labels to their corresponding sentiments (negative, positive, neutral).\n",
    "- Add instruction for each data entry, which is crucial for Instruction Tuning.\n",
    "- Convert the Pandas dataframe back to a Hugging Face Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "\n",
    "dic = {\n",
    "    0:\"negative\",\n",
    "    1:'positive',\n",
    "    2:'neutral',\n",
    "}\n",
    "\n",
    "# Load the TFNS dataset, apply the label mapping and rename the columns\n",
    "tfns = load_dataset('zeroshot/twitter-financial-news-sentiment')\n",
    "tfns = tfns['train']\n",
    "tfns = tfns.to_pandas()\n",
    "tfns['label'] = tfns['label'].apply(lambda x:dic[x])\n",
    "tfns['instruction'] = 'What is the sentiment of this tweet? Please choose an answer from {negative/neutral/positive}.'\n",
    "tfns.columns = ['input', 'output', 'instruction']\n",
    "tfns = datasets.Dataset.from_pandas(tfns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Concatenate and Shuffle the data\n",
    "- Concatenate TFNS into training set\n",
    "- Shuffle training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataset with itself to increase the number of examples\n",
    "tmp_dataset = datasets.concatenate_datasets([tfns]*2)\n",
    "train_dataset = tmp_dataset\n",
    "print(tmp_dataset.num_rows)\n",
    "\n",
    "# Shuffle the dataset\n",
    "all_dataset = train_dataset.shuffle(seed = 42)\n",
    "all_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Formatting and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from notebook.tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the dataset into the format of \"context\" and \"target\"\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into the list\n",
    "data_list = []\n",
    "for item in all_dataset.to_pandas().itertuples():\n",
    "    tmp = {}\n",
    "    tmp[\"instruction\"] = item.instruction\n",
    "    tmp[\"input\"] = item.input\n",
    "    tmp[\"output\"] = item.output\n",
    "    data_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the dataset into 5 disjoint subsets\n",
    "partition = []\n",
    "partition_size = 5\n",
    "for i in range(partition_size):\n",
    "    partition.append(data_list[i::partition_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a jsonl file\n",
    "with open(\"./data/dataset_new.jsonl\", 'w') as f:\n",
    "    for example in tqdm(data_list, desc=\"formatting..\"):\n",
    "        f.write(json.dumps(format_example(example)) + '\\n')\n",
    "\n",
    "# save the partition to a jsonl file\n",
    "for i in range(partition_size):\n",
    "    with open(f\"./data/dataset_new_partition_{i}.jsonl\", 'w') as f:\n",
    "        for example in tqdm(partition[i], desc=\"formatting..\"):\n",
    "            f.write(json.dumps(format_example(example)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization\n",
    "Tokenization is the process of converting input text into tokens that can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "jsonl_path = \"./data/dataset_new.jsonl\"  # updated path\n",
    "save_path = './data/dataset_new'  # updated path\n",
    "partition_path = './data/partition'  # updated path\n",
    "\n",
    "max_seq_length = 512\n",
    "skip_overlength = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocess function tokenizes the prompt and target, combines them into input IDs,\n",
    "# and then trims or pads the sequence to the maximum sequence length.\n",
    "def preprocess(tokenizer, config, example, max_seq_length):\n",
    "    prompt = example[\"context\"]\n",
    "    target = example[\"target\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, max_length=max_seq_length, truncation=True)\n",
    "    target_ids = tokenizer.encode(\n",
    "        target,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False)\n",
    "    input_ids = prompt_ids + target_ids + [config.eos_token_id]\n",
    "    return {\"input_ids\": input_ids, \"seq_len\": len(prompt_ids)}\n",
    "\n",
    "# The read_jsonl function reads each line from the JSONL file, preprocesses it using the preprocess function,\n",
    "# and then yields each preprocessed example.\n",
    "def read_jsonl(path, max_seq_length, skip_overlength=False):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, trust_remote_code=True)\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name, trust_remote_code=True, device_map='auto')\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            example = json.loads(line)\n",
    "            feature = preprocess(tokenizer, config, example, max_seq_length)\n",
    "            if skip_overlength and len(feature[\"input_ids\"]) > max_seq_length:\n",
    "                continue\n",
    "            feature[\"input_ids\"] = feature[\"input_ids\"][:max_seq_length]\n",
    "            yield feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The script then creates a Hugging Face Dataset object from the generator and saves it to disk.\n",
    "save_path = './data/dataset_new'\n",
    "\n",
    "dataset = datasets.Dataset.from_generator(\n",
    "    lambda: read_jsonl(jsonl_path, max_seq_length, skip_overlength)\n",
    "    )\n",
    "dataset.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(partition_size):\n",
    "    partition_path = f'./data/partition/partition_{i}'\n",
    "    dataset = datasets.Dataset.from_generator(\n",
    "        lambda: read_jsonl(f'./data/dataset_new_partition_{i}.jsonl', max_seq_length, skip_overlength)\n",
    "    )\n",
    "    dataset.save_to_disk(partition_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Setup FinGPT Training configuration with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Argument Setup\n",
    "Initialize and set training argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from loguru import logger\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    TaskType,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    set_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./finetuned_model',    # saved model path\n",
    "        logging_steps = 500,\n",
    "        # max_steps=10000,\n",
    "        num_train_epochs = 3,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=1000,\n",
    "        save_steps=500,\n",
    "        fp16=True,\n",
    "        # bf16=True,\n",
    "        torch_compile = False,\n",
    "        load_best_model_at_end = True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        remove_unused_columns=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Quantization Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization\n",
    "compute_dtype = getattr(torch, 'float16')\n",
    "q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=compute_dtype\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Loading & Preparation\n",
    "Load the base model and tokenizer, and prepare the model for kbit training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            trust_remote_code=True, \n",
    "                                            quantization_config=q_config, \n",
    "                                            device='cuda')\n",
    "model = prepare_model_for_kbit_training(model, q_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 LoRA Config & Setup\n",
    "Implement LoRA and print trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA\n",
    "target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['llama']\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules,\n",
    "    bias='none',\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint = None\n",
    "if resume_from_checkpoint is not None:\n",
    "    checkpoint_name = os.path.join(resume_from_checkpoint, 'pytorch_model.bin')\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, 'adapter_model.bin'\n",
    "        )\n",
    "        resume_from_checkpoint = False\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        logger.info(f'Restarting from {checkpoint_name}')\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        logger.info(f'Checkpoint {checkpoint_name} not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Copy the model 5 times\n",
    "model_list = [copy.deepcopy(model) for _ in range(partition_size)]\n",
    "for i in range(partition_size):\n",
    "    model_list[i].print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Loading Data and Training Llama2\n",
    "In this segment, we'll delve into the loading of your pre-processed data, and finally, launch the training of your FinGPT model. \n",
    "\n",
    "*Note: The federated model would also be trained in this step*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Loading your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from datasets import load_from_disk\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.load_from_disk(\"./data/dataset_new\")\n",
    "dataset = dataset.train_test_split(0.2, shuffle=True, seed = 42)\n",
    "\n",
    "# load the partition\n",
    "partitioned_dataset = []\n",
    "for i in range(partition_size):\n",
    "    partition_path = f'./data/partition/partition_{i}'\n",
    "    partitioned_dataset.append(datasets.load_from_disk(partition_path))\n",
    "    partitioned_dataset[i] = partitioned_dataset[i].train_test_split(0.2, shuffle=True, seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Configuration and Launch:\n",
    "- Customize the Trainer class for specific loss computation, prediction step, and model-saving methods.\n",
    "- Define a data collator function to process batches of data during training.\n",
    "- Set up TensorBoard for logging, instantiate your modified trainer, and begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss\n",
    "\n",
    "    def prediction_step(self, model: torch.nn.Module, inputs, prediction_loss_only: bool, ignore_keys = None):\n",
    "        with torch.no_grad():\n",
    "            res = model(\n",
    "                input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "                labels=inputs[\"labels\"].to(model.device),\n",
    "            ).loss\n",
    "        return (res, None, None)\n",
    "\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        from transformers.trainer import TRAINING_ARGS_NAME\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "        saved_params = {\n",
    "            k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n",
    "        }\n",
    "        torch.save(saved_params, os.path.join(output_dir, \"adapter_model.bin\"))\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [tokenizer.pad_token_id] * (seq_len - 1) + ids[(seq_len - 1) :] + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "writer = SummaryWriter()\n",
    "trainer = ModifiedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[TensorBoardCallback(writer)],\n",
    ")\n",
    "trainer.train()\n",
    "writer.close()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(training_args.output_dir)\n",
    "\n",
    "validation_loss = []\n",
    "\n",
    "# Train the partitioned models\n",
    "for i in range(partition_size):\n",
    "    writer = SummaryWriter()\n",
    "    partition_path = f'./finetuned_model/partitioned/partition_{i}'\n",
    "    partitioned_trainer = ModifiedTrainer(\n",
    "        model=model_list[i],\n",
    "        args=training_args,\n",
    "        train_dataset=partitioned_dataset[i][\"train\"],\n",
    "        eval_dataset=partitioned_dataset[i][\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[TensorBoardCallback(writer)],\n",
    "    )\n",
    "    partitioned_trainer.train()\n",
    "    writer.close()\n",
    "    \n",
    "    # Save the partitioned models\n",
    "    model_list[i].save_pretrained(partition_path)\n",
    "    \n",
    "    # Log validation loss\n",
    "    validation_loss = partitioned_trainer.evaluate()[\"eval_loss\"]\n",
    "    validation_loss.append(validation_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Calculate the weights for the partitioned models\n",
    "weights = [1 / loss for loss in validation_loss]\n",
    "total_weight = sum(weights)\n",
    "normalized_weights = [weight / total_weight for weight in weights]\n",
    "\n",
    "# Merge the partitioned adapters\n",
    "partition_path = f'./finetuned_model/partitioned/partition_0'\n",
    "aggregated_model = PeftModel.from_pretrained(model, partition_path, adapter_name='Institute_0')\n",
    "\n",
    "for i in range(1, partition_size):\n",
    "    partition_path = f'./finetuned_model/partitioned/partition_{i}'\n",
    "    _ = aggregated_model.load_adapter(partition_path, adapter_name=f'Institute_{i}')\n",
    "\n",
    "\n",
    "adapter_list = [f'Institute_{i}' for i in range(partition_size)]\n",
    "adapter_name = 'merge'\n",
    "density = 0.2\n",
    "aggregated_model.add_weighted_adapter(adapter_list, normalized_weights, \n",
    "                                      adapter_name, combination_type='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_model.set_adapter(adapter_name)\n",
    "\n",
    "# Save the aggregated model\n",
    "aggregated_path = './finetuned_model/aggregated'\n",
    "aggregated_model.save_pretrained()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Inference and Benchmarks using FinGPT\n",
    "Now that your model is trained, let’s understand how to use it to infer and run benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/FinNLP')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
