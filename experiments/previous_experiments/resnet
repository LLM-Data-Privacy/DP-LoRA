digraph {
	graph [size="71.55,71.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1956599365712 [label="
 (32, 10)" fillcolor=darkolivegreen1]
	1956599001680 [label=AddmmBackward0]
	1956599003936 -> 1956599001680
	1956599050416 [label="fc.bias
 (10)" fillcolor=lightblue]
	1956599050416 -> 1956599003936
	1956599003936 [label=AccumulateGrad]
	1956599003888 -> 1956599001680
	1956599003888 [label=ViewBackward0]
	1956598989440 -> 1956599003888
	1956598989440 [label=AvgPool2DBackward0]
	1956599001824 -> 1956598989440
	1956599001824 [label=ReluBackward0]
	1956599001008 -> 1956599001824
	1956599001008 [label=AddBackward0]
	1956599001104 -> 1956599001008
	1956599001104 [label=NativeBatchNormBackward0]
	1956598998368 -> 1956599001104
	1956598998368 [label=ConvolutionBackward0]
	1956598998608 -> 1956598998368
	1956598998608 [label=ReluBackward0]
	1956598988912 -> 1956598998608
	1956598988912 [label=NativeBatchNormBackward0]
	1956599002016 -> 1956598988912
	1956599002016 [label=ConvolutionBackward0]
	1956599001152 -> 1956599002016
	1956599001152 [label=ReluBackward0]
	1956599000384 -> 1956599001152
	1956599000384 [label=AddBackward0]
	1956599000624 -> 1956599000384
	1956599000624 [label=NativeBatchNormBackward0]
	1956598989392 -> 1956599000624
	1956598989392 [label=ConvolutionBackward0]
	1956598989152 -> 1956598989392
	1956598989152 [label=ReluBackward0]
	1956599002208 -> 1956598989152
	1956599002208 [label=NativeBatchNormBackward0]
	1956599002160 -> 1956599002208
	1956599002160 [label=ConvolutionBackward0]
	1956599002352 -> 1956599002160
	1956599002352 [label=ReluBackward0]
	1956599002688 -> 1956599002352
	1956599002688 [label=AddBackward0]
	1956599002640 -> 1956599002688
	1956599002640 [label=NativeBatchNormBackward0]
	1956598991600 -> 1956599002640
	1956598991600 [label=ConvolutionBackward0]
	1956599003984 -> 1956598991600
	1956599003984 [label=ReluBackward0]
	1956599003792 -> 1956599003984
	1956599003792 [label=NativeBatchNormBackward0]
	1956598990256 -> 1956599003792
	1956598990256 [label=ConvolutionBackward0]
	1956599002736 -> 1956598990256
	1956599002736 [label=ReluBackward0]
	1956598991312 -> 1956599002736
	1956598991312 [label=AddBackward0]
	1956598991360 -> 1956598991312
	1956598991360 [label=NativeBatchNormBackward0]
	1956598990880 -> 1956598991360
	1956598990880 [label=ConvolutionBackward0]
	1956598991072 -> 1956598990880
	1956598991072 [label=ReluBackward0]
	1956598996496 -> 1956598991072
	1956598996496 [label=NativeBatchNormBackward0]
	1956599001776 -> 1956598996496
	1956599001776 [label=ConvolutionBackward0]
	1956599000960 -> 1956599001776
	1956599000960 [label=ReluBackward0]
	1956599002832 -> 1956599000960
	1956599002832 [label=AddBackward0]
	1956598996208 -> 1956599002832
	1956598996208 [label=NativeBatchNormBackward0]
	1956598996064 -> 1956598996208
	1956598996064 [label=ConvolutionBackward0]
	1956598995920 -> 1956598996064
	1956598995920 [label=ReluBackward0]
	1956598996256 -> 1956598995920
	1956598996256 [label=NativeBatchNormBackward0]
	1956598995584 -> 1956598996256
	1956598995584 [label=ConvolutionBackward0]
	1956598996352 -> 1956598995584
	1956598996352 [label=ReluBackward0]
	1956598995296 -> 1956598996352
	1956598995296 [label=AddBackward0]
	1956598995536 -> 1956598995296
	1956598995536 [label=NativeBatchNormBackward0]
	1956598994960 -> 1956598995536
	1956598994960 [label=ConvolutionBackward0]
	1956598994768 -> 1956598994960
	1956598994768 [label=ReluBackward0]
	1956598994912 -> 1956598994768
	1956598994912 [label=NativeBatchNormBackward0]
	1956598994864 -> 1956598994912
	1956598994864 [label=ConvolutionBackward0]
	1956598993904 -> 1956598994864
	1956598993904 [label=ReluBackward0]
	1956598994000 -> 1956598993904
	1956598994000 [label=AddBackward0]
	1956598994048 -> 1956598994000
	1956598994048 [label=NativeBatchNormBackward0]
	1956598993856 -> 1956598994048
	1956598993856 [label=ConvolutionBackward0]
	1956598993568 -> 1956598993856
	1956598993568 [label=ReluBackward0]
	1956598993760 -> 1956598993568
	1956598993760 [label=NativeBatchNormBackward0]
	1956598993424 -> 1956598993760
	1956598993424 [label=ConvolutionBackward0]
	1956598994144 -> 1956598993424
	1956598994144 [label=ReluBackward0]
	1956598998656 -> 1956598994144
	1956598998656 [label=AddBackward0]
	1956599000000 -> 1956598998656
	1956599000000 [label=NativeBatchNormBackward0]
	1956598992608 -> 1956599000000
	1956598992608 [label=ConvolutionBackward0]
	1956598992656 -> 1956598992608
	1956598992656 [label=ReluBackward0]
	1956598992800 -> 1956598992656
	1956598992800 [label=NativeBatchNormBackward0]
	1956598992752 -> 1956598992800
	1956598992752 [label=ConvolutionBackward0]
	1956598993328 -> 1956598992752
	1956598993328 [label=MaxPool2DWithIndicesBackward0]
	1956599003600 -> 1956598993328
	1956599003600 [label=ReluBackward0]
	1956599003552 -> 1956599003600
	1956599003552 [label=NativeBatchNormBackward0]
	1956599003408 -> 1956599003552
	1956599003408 [label=ConvolutionBackward0]
	1956599003120 -> 1956599003408
	1956598691984 [label="conv1.0.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1956598691984 -> 1956599003120
	1956599003120 [label=AccumulateGrad]
	1956599003168 -> 1956599003408
	1956598690640 [label="conv1.0.bias
 (64)" fillcolor=lightblue]
	1956598690640 -> 1956599003168
	1956599003168 [label=AccumulateGrad]
	1956599003456 -> 1956599003552
	1956598689776 [label="conv1.1.weight
 (64)" fillcolor=lightblue]
	1956598689776 -> 1956599003456
	1956599003456 [label=AccumulateGrad]
	1956599003312 -> 1956599003552
	1956598690832 [label="conv1.1.bias
 (64)" fillcolor=lightblue]
	1956598690832 -> 1956599003312
	1956599003312 [label=AccumulateGrad]
	1956598990784 -> 1956598992752
	1956598692368 [label="layer0.0.conv1.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1956598692368 -> 1956598990784
	1956598990784 [label=AccumulateGrad]
	1956599005040 -> 1956598992752
	1956598692464 [label="layer0.0.conv1.0.bias
 (64)" fillcolor=lightblue]
	1956598692464 -> 1956599005040
	1956599005040 [label=AccumulateGrad]
	1956598992896 -> 1956598992800
	1956598692560 [label="layer0.0.conv1.1.weight
 (64)" fillcolor=lightblue]
	1956598692560 -> 1956598992896
	1956598992896 [label=AccumulateGrad]
	1956599000192 -> 1956598992800
	1956598692656 [label="layer0.0.conv1.1.bias
 (64)" fillcolor=lightblue]
	1956598692656 -> 1956599000192
	1956599000192 [label=AccumulateGrad]
	1956598992992 -> 1956598992608
	1956598693040 [label="layer0.0.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1956598693040 -> 1956598992992
	1956598992992 [label=AccumulateGrad]
	1956598991840 -> 1956598992608
	1956598693136 [label="layer0.0.conv2.0.bias
 (64)" fillcolor=lightblue]
	1956598693136 -> 1956598991840
	1956598991840 [label=AccumulateGrad]
	1956598993040 -> 1956599000000
	1956598693232 [label="layer0.0.conv2.1.weight
 (64)" fillcolor=lightblue]
	1956598693232 -> 1956598993040
	1956598993040 [label=AccumulateGrad]
	1956598998704 -> 1956599000000
	1956598693328 [label="layer0.0.conv2.1.bias
 (64)" fillcolor=lightblue]
	1956598693328 -> 1956598998704
	1956598998704 [label=AccumulateGrad]
	1956598993328 -> 1956598998656
	1956598993376 -> 1956598993424
	1956599038896 [label="layer0.1.conv1.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1956599038896 -> 1956598993376
	1956598993376 [label=AccumulateGrad]
	1956598993184 -> 1956598993424
	1956599038992 [label="layer0.1.conv1.0.bias
 (64)" fillcolor=lightblue]
	1956599038992 -> 1956598993184
	1956598993184 [label=AccumulateGrad]
	1956598993616 -> 1956598993760
	1956599039088 [label="layer0.1.conv1.1.weight
 (64)" fillcolor=lightblue]
	1956599039088 -> 1956598993616
	1956598993616 [label=AccumulateGrad]
	1956598993136 -> 1956598993760
	1956599039184 [label="layer0.1.conv1.1.bias
 (64)" fillcolor=lightblue]
	1956599039184 -> 1956598993136
	1956598993136 [label=AccumulateGrad]
	1956598993472 -> 1956598993856
	1956599039568 [label="layer0.1.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1956599039568 -> 1956598993472
	1956598993472 [label=AccumulateGrad]
	1956598993520 -> 1956598993856
	1956599039664 [label="layer0.1.conv2.0.bias
 (64)" fillcolor=lightblue]
	1956599039664 -> 1956598993520
	1956598993520 [label=AccumulateGrad]
	1956598994384 -> 1956598994048
	1956599039760 [label="layer0.1.conv2.1.weight
 (64)" fillcolor=lightblue]
	1956599039760 -> 1956598994384
	1956598994384 [label=AccumulateGrad]
	1956598994432 -> 1956598994048
	1956599039856 [label="layer0.1.conv2.1.bias
 (64)" fillcolor=lightblue]
	1956599039856 -> 1956598994432
	1956598994432 [label=AccumulateGrad]
	1956598994144 -> 1956598994000
	1956598993952 -> 1956598994864
	1956599040912 [label="layer1.0.conv1.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1956599040912 -> 1956598993952
	1956598993952 [label=AccumulateGrad]
	1956598994336 -> 1956598994864
	1956599041008 [label="layer1.0.conv1.0.bias
 (128)" fillcolor=lightblue]
	1956599041008 -> 1956598994336
	1956598994336 [label=AccumulateGrad]
	1956598994624 -> 1956598994912
	1956599041104 [label="layer1.0.conv1.1.weight
 (128)" fillcolor=lightblue]
	1956599041104 -> 1956598994624
	1956598994624 [label=AccumulateGrad]
	1956598995008 -> 1956598994912
	1956599041200 [label="layer1.0.conv1.1.bias
 (128)" fillcolor=lightblue]
	1956599041200 -> 1956598995008
	1956598995008 [label=AccumulateGrad]
	1956598994816 -> 1956598994960
	1956599041584 [label="layer1.0.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1956599041584 -> 1956598994816
	1956598994816 [label=AccumulateGrad]
	1956598994528 -> 1956598994960
	1956599041680 [label="layer1.0.conv2.0.bias
 (128)" fillcolor=lightblue]
	1956599041680 -> 1956598994528
	1956598994528 [label=AccumulateGrad]
	1956598995104 -> 1956598995536
	1956599041776 [label="layer1.0.conv2.1.weight
 (128)" fillcolor=lightblue]
	1956599041776 -> 1956598995104
	1956598995104 [label=AccumulateGrad]
	1956598995632 -> 1956598995536
	1956599041872 [label="layer1.0.conv2.1.bias
 (128)" fillcolor=lightblue]
	1956599041872 -> 1956598995632
	1956598995632 [label=AccumulateGrad]
	1956598995344 -> 1956598995296
	1956598995344 [label=NativeBatchNormBackward0]
	1956598994672 -> 1956598995344
	1956598994672 [label=ConvolutionBackward0]
	1956598993904 -> 1956598994672
	1956598994240 -> 1956598994672
	1956599040240 [label="layer1.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1956599040240 -> 1956598994240
	1956598994240 [label=AccumulateGrad]
	1956598994096 -> 1956598994672
	1956599040336 [label="layer1.0.downsample.0.bias
 (128)" fillcolor=lightblue]
	1956599040336 -> 1956598994096
	1956598994096 [label=AccumulateGrad]
	1956598994720 -> 1956598995344
	1956599040432 [label="layer1.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1956599040432 -> 1956598994720
	1956598994720 [label=AccumulateGrad]
	1956598994576 -> 1956598995344
	1956599040528 [label="layer1.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1956599040528 -> 1956598994576
	1956598994576 [label=AccumulateGrad]
	1956598995392 -> 1956598995584
	1956599042256 [label="layer1.1.conv1.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1956599042256 -> 1956598995392
	1956598995392 [label=AccumulateGrad]
	1956598995440 -> 1956598995584
	1956599042352 [label="layer1.1.conv1.0.bias
 (128)" fillcolor=lightblue]
	1956599042352 -> 1956598995440
	1956598995440 [label=AccumulateGrad]
	1956598995728 -> 1956598996256
	1956599042448 [label="layer1.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	1956599042448 -> 1956598995728
	1956598995728 [label=AccumulateGrad]
	1956598996112 -> 1956598996256
	1956599042544 [label="layer1.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	1956599042544 -> 1956598996112
	1956598996112 [label=AccumulateGrad]
	1956598995872 -> 1956598996064
	1956599042928 [label="layer1.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1956599042928 -> 1956598995872
	1956598995872 [label=AccumulateGrad]
	1956598995968 -> 1956598996064
	1956599043024 [label="layer1.1.conv2.0.bias
 (128)" fillcolor=lightblue]
	1956599043024 -> 1956598995968
	1956598995968 [label=AccumulateGrad]
	1956598995776 -> 1956598996208
	1956599043120 [label="layer1.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	1956599043120 -> 1956598995776
	1956598995776 [label=AccumulateGrad]
	1956598995824 -> 1956598996208
	1956599043216 [label="layer1.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	1956599043216 -> 1956598995824
	1956598995824 [label=AccumulateGrad]
	1956598996352 -> 1956599002832
	1956599001056 -> 1956599001776
	1956599044272 [label="layer2.0.conv1.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1956599044272 -> 1956599001056
	1956599001056 [label=AccumulateGrad]
	1956599001200 -> 1956599001776
	1956599044368 [label="layer2.0.conv1.0.bias
 (256)" fillcolor=lightblue]
	1956599044368 -> 1956599001200
	1956599001200 [label=AccumulateGrad]
	1956598996448 -> 1956598996496
	1956599044464 [label="layer2.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	1956599044464 -> 1956598996448
	1956598996448 [label=AccumulateGrad]
	1956598996592 -> 1956598996496
	1956599044560 [label="layer2.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	1956599044560 -> 1956598996592
	1956598996592 [label=AccumulateGrad]
	1956598990928 -> 1956598990880
	1956599044944 [label="layer2.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1956599044944 -> 1956598990928
	1956598990928 [label=AccumulateGrad]
	1956598990976 -> 1956598990880
	1956599045040 [label="layer2.0.conv2.0.bias
 (256)" fillcolor=lightblue]
	1956599045040 -> 1956598990976
	1956598990976 [label=AccumulateGrad]
	1956598990496 -> 1956598991360
	1956599045136 [label="layer2.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	1956599045136 -> 1956598990496
	1956598990496 [label=AccumulateGrad]
	1956598990544 -> 1956598991360
	1956599045232 [label="layer2.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	1956599045232 -> 1956598990544
	1956598990544 [label=AccumulateGrad]
	1956598991264 -> 1956598991312
	1956598991264 [label=NativeBatchNormBackward0]
	1956598996544 -> 1956598991264
	1956598996544 [label=ConvolutionBackward0]
	1956599000960 -> 1956598996544
	1956599000864 -> 1956598996544
	1956599043600 [label="layer2.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1956599043600 -> 1956599000864
	1956599000864 [label=AccumulateGrad]
	1956598998080 -> 1956598996544
	1956599043696 [label="layer2.0.downsample.0.bias
 (256)" fillcolor=lightblue]
	1956599043696 -> 1956598998080
	1956598998080 [label=AccumulateGrad]
	1956598991024 -> 1956598991264
	1956599043792 [label="layer2.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1956599043792 -> 1956598991024
	1956598991024 [label=AccumulateGrad]
	1956598990592 -> 1956598991264
	1956599043888 [label="layer2.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1956599043888 -> 1956598990592
	1956598990592 [label=AccumulateGrad]
	1956598991504 -> 1956598990256
	1956599045616 [label="layer2.1.conv1.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1956599045616 -> 1956598991504
	1956598991504 [label=AccumulateGrad]
	1956598990352 -> 1956598990256
	1956599045712 [label="layer2.1.conv1.0.bias
 (256)" fillcolor=lightblue]
	1956599045712 -> 1956598990352
	1956598990352 [label=AccumulateGrad]
	1956598991552 -> 1956599003792
	1956599045808 [label="layer2.1.conv1.1.weight
 (256)" fillcolor=lightblue]
	1956599045808 -> 1956598991552
	1956598991552 [label=AccumulateGrad]
	1956599003696 -> 1956599003792
	1956599045904 [label="layer2.1.conv1.1.bias
 (256)" fillcolor=lightblue]
	1956599045904 -> 1956599003696
	1956599003696 [label=AccumulateGrad]
	1956599004128 -> 1956598991600
	1956599046288 [label="layer2.1.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1956599046288 -> 1956599004128
	1956599004128 [label=AccumulateGrad]
	1956599004080 -> 1956598991600
	1956599046384 [label="layer2.1.conv2.0.bias
 (256)" fillcolor=lightblue]
	1956599046384 -> 1956599004080
	1956599004080 [label=AccumulateGrad]
	1956598991648 -> 1956599002640
	1956599046480 [label="layer2.1.conv2.1.weight
 (256)" fillcolor=lightblue]
	1956599046480 -> 1956598991648
	1956598991648 [label=AccumulateGrad]
	1956599002784 -> 1956599002640
	1956599046576 [label="layer2.1.conv2.1.bias
 (256)" fillcolor=lightblue]
	1956599046576 -> 1956599002784
	1956599002784 [label=AccumulateGrad]
	1956599002736 -> 1956599002688
	1956599002544 -> 1956599002160
	1956599047632 [label="layer3.0.conv1.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1956599047632 -> 1956599002544
	1956599002544 [label=AccumulateGrad]
	1956599002496 -> 1956599002160
	1956599047728 [label="layer3.0.conv1.0.bias
 (512)" fillcolor=lightblue]
	1956599047728 -> 1956599002496
	1956599002496 [label=AccumulateGrad]
	1956599002256 -> 1956599002208
	1956599047824 [label="layer3.0.conv1.1.weight
 (512)" fillcolor=lightblue]
	1956599047824 -> 1956599002256
	1956599002256 [label=AccumulateGrad]
	1956598989488 -> 1956599002208
	1956599047920 [label="layer3.0.conv1.1.bias
 (512)" fillcolor=lightblue]
	1956599047920 -> 1956598989488
	1956598989488 [label=AccumulateGrad]
	1956598989200 -> 1956598989392
	1956599048304 [label="layer3.0.conv2.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1956599048304 -> 1956598989200
	1956598989200 [label=AccumulateGrad]
	1956598989296 -> 1956598989392
	1956599048400 [label="layer3.0.conv2.0.bias
 (512)" fillcolor=lightblue]
	1956599048400 -> 1956598989296
	1956598989296 [label=AccumulateGrad]
	1956599000672 -> 1956599000624
	1956599048496 [label="layer3.0.conv2.1.weight
 (512)" fillcolor=lightblue]
	1956599048496 -> 1956599000672
	1956599000672 [label=AccumulateGrad]
	1956599000720 -> 1956599000624
	1956599048592 [label="layer3.0.conv2.1.bias
 (512)" fillcolor=lightblue]
	1956599048592 -> 1956599000720
	1956599000720 [label=AccumulateGrad]
	1956599000528 -> 1956599000384
	1956599000528 [label=NativeBatchNormBackward0]
	1956598989104 -> 1956599000528
	1956598989104 [label=ConvolutionBackward0]
	1956599002352 -> 1956598989104
	1956599002448 -> 1956598989104
	1956599046960 [label="layer3.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1956599046960 -> 1956599002448
	1956599002448 [label=AccumulateGrad]
	1956599002592 -> 1956598989104
	1956599047056 [label="layer3.0.downsample.0.bias
 (512)" fillcolor=lightblue]
	1956599047056 -> 1956599002592
	1956599002592 [label=AccumulateGrad]
	1956598989536 -> 1956599000528
	1956599047152 [label="layer3.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1956599047152 -> 1956598989536
	1956598989536 [label=AccumulateGrad]
	1956598989344 -> 1956599000528
	1956599047248 [label="layer3.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1956599047248 -> 1956598989344
	1956598989344 [label=AccumulateGrad]
	1956599000576 -> 1956599002016
	1956599048976 [label="layer3.1.conv1.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1956599048976 -> 1956599000576
	1956599000576 [label=AccumulateGrad]
	1956599000336 -> 1956599002016
	1956599049072 [label="layer3.1.conv1.0.bias
 (512)" fillcolor=lightblue]
	1956599049072 -> 1956599000336
	1956599000336 [label=AccumulateGrad]
	1956599001968 -> 1956598988912
	1956599049168 [label="layer3.1.conv1.1.weight
 (512)" fillcolor=lightblue]
	1956599049168 -> 1956599001968
	1956599001968 [label=AccumulateGrad]
	1956599000240 -> 1956598988912
	1956599049264 [label="layer3.1.conv1.1.bias
 (512)" fillcolor=lightblue]
	1956599049264 -> 1956599000240
	1956599000240 [label=AccumulateGrad]
	1956598998560 -> 1956598998368
	1956599049648 [label="layer3.1.conv2.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1956599049648 -> 1956598998560
	1956598998560 [label=AccumulateGrad]
	1956598998512 -> 1956598998368
	1956599049744 [label="layer3.1.conv2.0.bias
 (512)" fillcolor=lightblue]
	1956599049744 -> 1956598998512
	1956598998512 [label=AccumulateGrad]
	1956598996832 -> 1956599001104
	1956599049840 [label="layer3.1.conv2.1.weight
 (512)" fillcolor=lightblue]
	1956599049840 -> 1956598996832
	1956598996832 [label=AccumulateGrad]
	1956599000768 -> 1956599001104
	1956599049936 [label="layer3.1.conv2.1.bias
 (512)" fillcolor=lightblue]
	1956599049936 -> 1956599000768
	1956599000768 [label=AccumulateGrad]
	1956599001152 -> 1956599001008
	1956599003840 -> 1956599001680
	1956599003840 [label=TBackward0]
	1956599001872 -> 1956599003840
	1956599050320 [label="fc.weight
 (10, 512)" fillcolor=lightblue]
	1956599050320 -> 1956599001872
	1956599001872 [label=AccumulateGrad]
	1956599001680 -> 1956599365712
}
