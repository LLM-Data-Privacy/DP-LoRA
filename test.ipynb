{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "Loading data from **MNist** dataset, and define image embeddings \\\\\n",
    "\n",
    "In this case, we have two separated datasets, the unpartitioned and partitioned. We perform traditional training on unpartitioned dataset and perform federated learning on partitioned dataset. The partitioned dataset would be splitted into 10 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations to apply to the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Define the basic CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model, trainloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n",
    "\n",
    "# Test function\n",
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy on test set: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training non-partitioned model...\n",
      "Epoch 1, Loss: 0.12053956443245212\n",
      "Epoch 2, Loss: 0.04296070709419437\n",
      "Epoch 3, Loss: 0.029533452758398682\n",
      "Epoch 4, Loss: 0.020725786873478987\n",
      "Epoch 5, Loss: 0.016609743203362934\n",
      "Testing non-partitioned model...\n",
      "Accuracy on test set: 99.08%\n"
     ]
    }
   ],
   "source": [
    "# Train non-partitioned model\n",
    "non_partitioned_model = CNN()\n",
    "non_partitioned_optimizer = optim.Adam(non_partitioned_model.parameters(), lr=0.001)\n",
    "non_partitioned_criterion = nn.CrossEntropyLoss()\n",
    "print(\"Training non-partitioned model...\")\n",
    "train(non_partitioned_model, trainloader, non_partitioned_criterion, non_partitioned_optimizer)\n",
    "print(\"Testing non-partitioned model...\")\n",
    "test(non_partitioned_model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.5389743443935159\n",
      "Epoch 2, Loss: 0.12819059653098674\n",
      "Epoch 3, Loss: 0.08596511012403929\n",
      "Epoch 4, Loss: 0.061944813763554346\n",
      "Epoch 5, Loss: 0.04039334639797899\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.5271242829535078\n",
      "Epoch 2, Loss: 0.13601120816484252\n",
      "Epoch 3, Loss: 0.08528326340812317\n",
      "Epoch 4, Loss: 0.058867816891932385\n",
      "Epoch 5, Loss: 0.046563847318967864\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.5548746767552927\n",
      "Epoch 2, Loss: 0.1298850610545103\n",
      "Epoch 3, Loss: 0.08472613878226146\n",
      "Epoch 4, Loss: 0.04199276243007068\n",
      "Epoch 5, Loss: 0.036628803658730134\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.5086175176572609\n",
      "Epoch 2, Loss: 0.12552474394559543\n",
      "Epoch 3, Loss: 0.07053496785947379\n",
      "Epoch 4, Loss: 0.05023680604620282\n",
      "Epoch 5, Loss: 0.029827385666957272\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.52920163478306\n",
      "Epoch 2, Loss: 0.11591915913827797\n",
      "Epoch 3, Loss: 0.07385448826863332\n",
      "Epoch 4, Loss: 0.04916729882825166\n",
      "Epoch 5, Loss: 0.03707197577410199\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.5489052963304393\n",
      "Epoch 2, Loss: 0.13021055486627875\n",
      "Epoch 3, Loss: 0.08248598851570661\n",
      "Epoch 4, Loss: 0.05225972872056523\n",
      "Epoch 5, Loss: 0.03767312103770971\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.5007162890297935\n",
      "Epoch 2, Loss: 0.12082519467444496\n",
      "Epoch 3, Loss: 0.08554040238092792\n",
      "Epoch 4, Loss: 0.0579892047773799\n",
      "Epoch 5, Loss: 0.0385915405473652\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.5430611947828785\n",
      "Epoch 2, Loss: 0.14112491337978778\n",
      "Epoch 3, Loss: 0.09122865522736089\n",
      "Epoch 4, Loss: 0.05808537041748616\n",
      "Epoch 5, Loss: 0.03457809835325609\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.533849637519489\n",
      "Epoch 2, Loss: 0.1395375064891228\n",
      "Epoch 3, Loss: 0.08437563653074284\n",
      "Epoch 4, Loss: 0.05496076819303624\n",
      "Epoch 5, Loss: 0.03426374068255834\n",
      "Training partitioned model...\n",
      "Epoch 1, Loss: 0.4573025438003242\n",
      "Epoch 2, Loss: 0.10940738835253139\n",
      "Epoch 3, Loss: 0.06640255287725558\n",
      "Epoch 4, Loss: 0.03860739700680727\n",
      "Epoch 5, Loss: 0.026957004553046414\n",
      "Aggregating model updates...\n"
     ]
    }
   ],
   "source": [
    "# Train partitioned model (simulate federated learning)\n",
    "partition_size = len(trainset) // 10\n",
    "data_partitions = [torch.utils.data.Subset(trainset, range(i * partition_size, (i + 1) * partition_size))\n",
    "                   for i in range(10)]\n",
    "\n",
    "partitioned_models = []\n",
    "partitioned_optimizers = []\n",
    "for _ in range(10):\n",
    "    model = CNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(\"Training partitioned model...\")\n",
    "    train(model, torch.utils.data.DataLoader(data_partitions[_], batch_size=32, shuffle=True), criterion, optimizer)\n",
    "    partitioned_models.append(model)\n",
    "    partitioned_optimizers.append(optimizer)\n",
    "\n",
    "# Aggregate model updates\n",
    "print(\"Aggregating model updates...\")\n",
    "for i in range(1, 10):\n",
    "    for params_source, params_target in zip(partitioned_models[i].parameters(), partitioned_models[0].parameters()):\n",
    "        params_target.data += params_source.data\n",
    "\n",
    "# Average aggregated model parameters\n",
    "for params_target in partitioned_models[0].parameters():\n",
    "    params_target.data /= 10\n",
    "\n",
    "aggregated_model = partitioned_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing aggregated model...\n",
      "Accuracy on test set: 11.35%\n"
     ]
    }
   ],
   "source": [
    "# Test federated model\n",
    "print(\"Testing aggregated model...\")\n",
    "test(aggregated_model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
